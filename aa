#!/bin/bash

# Database connection parameters
DB_HOST="your_host"
DB_PORT="your_port"
DB_NAME="your_database"
DB_USER="your_user"
DB_PASSWORD="your_password"

# Table to be exported
TABLE_NAME="your_table_name"
SCHEMA_NAME="your_schema_name"

# Set PGPASSWORD environment variable
export PGPASSWORD=$DB_PASSWORD

# Output SQL file
OUTPUT_FILE="insert_queries.sql"

# Batch size for each query
BATCH_SIZE=100000

# Counter for naming split files
counter=1

# Using pg_dump to generate insert statements with LIMIT and OFFSET
OFFSET=0
while true; do
    pg_dump -h $DB_HOST -p $DB_PORT -U $DB_USER -d $DB_NAME --table=$SCHEMA_NAME.$TABLE_NAME --data-only --column-inserts --no-password --inserts --limit=$BATCH_SIZE --offset=$OFFSET >> $OUTPUT_FILE
    # Break the loop if the last query returned fewer rows than the batch size
    [ "$(psql -h $DB_HOST -p $DB_PORT -U $DB_USER -d $DB_NAME -tAc "SELECT COUNT(*) FROM $SCHEMA_NAME.$TABLE_NAME WHERE your_condition")" -lt $BATCH_SIZE ] && break
    OFFSET=$((OFFSET + BATCH_SIZE + 1))

    # Split the output file into smaller files with 100,000 lines each
    split -l 100000 $OUTPUT_FILE "file_$counter"
    
    # Move the split files to have a .sql extension
    for file in file_$counter*; do
        mv "$file" "$file.sql"
    done

    # Increment the counter
    counter=$((counter + 1))
done





#!/bin/bash

# Database connection parameters
DB_HOST="your_host"
DB_PORT="your_port"
DB_NAME="your_database"
DB_USER="your_user"
DB_PASSWORD="your_password"

# Table to be exported
TABLE_NAME="your_table_name"
SCHEMA_NAME="your_schema_name"

# Set PGPASSWORD environment variable
export PGPASSWORD=$DB_PASSWORD

# Output SQL file
OUTPUT_FILE="insert_queries.sql"

# Batch size for each query
BATCH_SIZE=100000

# Counter for naming split files
counter=1

# Using pg_dump to generate insert statements with LIMIT and OFFSET
OFFSET=0
while true; do
    pg_dump -h $DB_HOST -p $DB_PORT -U $DB_USER -d $DB_NAME --table=$SCHEMA_NAME.$TABLE_NAME --data-only --column-inserts --no-password --inserts --file=- | psql -h $DB_HOST -p $DB_PORT -U $DB_USER -d $DB_NAME --quiet --no-password --set ON_ERROR_STOP=on --set AUTOCOMMIT=off --single-transaction -c "COPY (SELECT * FROM $SCHEMA_NAME.$TABLE_NAME ORDER BY your_order_column OFFSET $OFFSET LIMIT $BATCH_SIZE) TO STDOUT" >> $OUTPUT_FILE

    # Break the loop if the last query returned fewer rows than the batch size
    [ "$(psql -h $DB_HOST -p $DB_PORT -U $DB_USER -d $DB_NAME -tAc "SELECT COUNT(*) FROM $SCHEMA_NAME.$TABLE_NAME WHERE your_condition")" -lt $BATCH_SIZE ] && break
    OFFSET=$((OFFSET + BATCH_SIZE))

    # Split the output file into smaller files with 100,000 lines each
    split -l 100000 $OUTPUT_FILE "file_$counter"
    
    # Move the split files to have a .sql extension
    for file in file_$counter*; do
        mv "$file" "$file.sql"
    done

    # Increment the counter
    counter=$((counter + 1))
done

#!/bin/bash

# URL to check
URL="https://getdetails.com"

# Check if the URL is up
HTTP_STATUS=$(curl --write-out "%{http_code}" --silent --output /dev/null "$URL")

# If the status code is 200, proceed with the main curl command
if [ "$HTTP_STATUS" -eq 200 ]; then
  echo "URL is up, proceeding with the main request."

  curl --location 'https://getdetails.com/update' \
  --header 'Content-Type: application/json' \
  --data '{ "database" : "ABC", "date" : "2024-06-25"}'
else
  echo "URL is not reachable. Status code: $HTTP_STATUS"
fi



="insert into demo_table (name, id, error_typ, cat_typ, sys_date_Time, wi_number) select name, id, error_typ, cat_typ, current_date, '" & C2 & "' from demo_table where wi_number=" & A2 & ";"

=MID(A1, 8, 2) & "/" & TEXT(MATCH(MID(A1, 10, 3), {"JAN","FEB","MAR","APR","MAY","JUN","JUL","AUG","SEP","OCT","NOV","DEC"},0), "00") & "/20" & MID(A1, 13, 2)




Here's the complete code that you need for your project:

Step 1: Create the .env file (e.g., test.env)

Inside your project folder, create a .env file with the following content:

bash
Copy code
project_key=application-spyder
project_name=spyder
Step 2: Install dotenv package

Run the following command in your project directory to install the dotenv package:

bash
Copy code
npm install dotenv


Step 3: Create the script to generate sonar-project.properties (e.g., generate-sonar-properties.js)

Inside your project folder, create a file named generate-sonar-properties.js with the following content:

javascript
Copy code
const fs = require('fs');
const dotenv = require('dotenv');

// Load the .env file
dotenv.config({ path: './test.env' });

// Create the sonar-project.properties content
const sonarProperties = `
sonar.projectKey=${process.env.project_key}
sonar.projectName=${process.env.project_name}
`;

// Write the content to sonar-project.properties
fs.writeFileSync('sonar-project.properties', sonarProperties, 'utf8');

console.log('sonar-project.properties file has been generated.');
Step 4: Update package.json scripts

Open your package.json file and update the "scripts" section like this:

json
Copy code
{
  "scripts": {
    "generate-sonar-properties": "node generate-sonar-properties.js"
  }
}
Step 5: Run the script

Whenever you need to generate the sonar-project.properties file, run the following command:

bash
Copy code
npm run generate-sonar-properties
This script will load the environment variables from the .env file and dynamically generate a sonar-project.properties file with the correct values. You can adjust the .env file for different environments without modifying your SonarQube configuration.




// emailNotification.test.js
const nodemailer = require('nodemailer');
const { createMailOptions, sendEmail } = require('./emailNotification');

jest.mock('nodemailer'); // Mock the nodemailer module

describe('Email Notification Tests', () => {
  let sendMailMock;

  beforeAll(() => {
    // Set up a mock transporter with a sendMail mock function
    sendMailMock = jest.fn().mockImplementation((mailOptions, callback) => {
      callback(null, 'Email sent');
    });
    nodemailer.createTransport.mockReturnValue({
      sendMail: sendMailMock,
    });
  });

  afterEach(() => {
    jest.clearAllMocks(); // Clear mocks after each test
  });

  test('createMailOptions should return correct mail options', () => {
    const subject = 'Test Subject';
    const text = 'Test Text';
    const html = '<p>Test HTML</p>';

    const mailOptions = createMailOptions(subject, text, html);

    expect(mailOptions).toEqual({
      from: 'demo@demo.com',
      to: 'demo@demo.com',
      subject: subject,
      text: text,
      html: html,
    });
  });

  test('sendEmail should call transporter.sendMail with correct options', async () => {
    const subject = 'Test Subject';
    const text = 'Test Text';
    const html = '<p>Test HTML</p>';

    await expect(sendEmail(subject, text, html)).resolves.toBe('Email sent');

    // Verify sendMail was called with the expected mail options
    expect(sendMailMock).toHaveBeenCalledWith(
      {
        from: 'demo@demo.com',
        to: 'demo@demo.com',
        subject: subject,
        text: text,
        html: html,
      },
      expect.any(Function)
    );
  });

  test('sendEmail should handle errors correctly', async () => {
    const errorMessage = 'Error sending email';
    sendMailMock.mockImplementationOnce((mailOptions, callback) => {
      callback(new Error(errorMessage), null);
    });

    await expect(sendEmail('Subject', 'Text', 'HTML')).rejects.toThrow(errorMessage);
  });
});


// emailNotification.js
const nodemailer = require('nodemailer');

const transporter = nodemailer.createTransport({
  host: 'host Details',
  port: '25',
  secure: false,
  tls: {
    rejectUnauthorized: false,
  },
});

const createMailOptions = (subject, text, html) => ({
  from: 'demo@demo.com',
  to: 'demo@demo.com',
  subject,
  text,
  html,
});

const sendEmail = (subject, text, html) => {
  const mailOptions = createMailOptions(subject, text, html);

  return new Promise((resolve, reject) => {
    transporter.sendMail(mailOptions, (error, info) => {
      if (error) {
        reject(error); // Reject if there's an error
      } else {
        resolve('Email sent'); // Resolve if the email was sent successfully
      }
    });
  });
};

module.exports = { sendEmail, createMailOptions };


nohup java -jar agent.jar -jnlpUrl http://your-vm-ip:8080/computer/agent-1/slave-agent.jnlp -secret <SECRET_KEY> > agent.log 2>&1 &

echo '<com.cloudbees.plugins.credentials.impl.FileCredentialsImpl>
  <scope>GLOBAL</scope>
  <id>minikube-kubeconfig</id>
  <description>Kubeconfig for Minikube</description>
  <fileName>kubeconfig.yaml</fileName>
  <secretBytes>'$(cat kubeconfig.yaml | base64)'</secretBytes>
</com.cloudbees.plugins.credentials.impl.FileCredentialsImpl>' | java -jar jenkins-cli.jar -s http://localhost:8080/ create-credentials-by-xml system::system::jenkins _ < /dev/stdin

<com.cloudbees.plugins.credentials.impl.UsernamePasswordCredentialsImpl>
    <scope>GLOBAL</scope>
    <id>my-k8s-credentials</id>
    <username>k8s-user</username>
    <password>k8s-password</password>
    <description>My Kubernetes Credentials</description>
</com.cloudbees.plugins.credentials.impl.UsernamePasswordCredentialsImpl>
