#!/bin/bash

# Database connection parameters
DB_HOST="your_host"
DB_PORT="your_port"
DB_NAME="your_database"
DB_USER="your_user"
DB_PASSWORD="your_password"

# Table to be exported
TABLE_NAME="your_table_name"
SCHEMA_NAME="your_schema_name"

# Set PGPASSWORD environment variable
export PGPASSWORD=$DB_PASSWORD

# Output SQL file
OUTPUT_FILE="insert_queries.sql"

# Batch size for each query
BATCH_SIZE=100000

# Counter for naming split files
counter=1

# Using pg_dump to generate insert statements with LIMIT and OFFSET
OFFSET=0
while true; do
    pg_dump -h $DB_HOST -p $DB_PORT -U $DB_USER -d $DB_NAME --table=$SCHEMA_NAME.$TABLE_NAME --data-only --column-inserts --no-password --inserts --limit=$BATCH_SIZE --offset=$OFFSET >> $OUTPUT_FILE
    # Break the loop if the last query returned fewer rows than the batch size
    [ "$(psql -h $DB_HOST -p $DB_PORT -U $DB_USER -d $DB_NAME -tAc "SELECT COUNT(*) FROM $SCHEMA_NAME.$TABLE_NAME WHERE your_condition")" -lt $BATCH_SIZE ] && break
    OFFSET=$((OFFSET + BATCH_SIZE + 1))

    # Split the output file into smaller files with 100,000 lines each
    split -l 100000 $OUTPUT_FILE "file_$counter"
    
    # Move the split files to have a .sql extension
    for file in file_$counter*; do
        mv "$file" "$file.sql"
    done

    # Increment the counter
    counter=$((counter + 1))
done





#!/bin/bash

# Database connection parameters
DB_HOST="your_host"
DB_PORT="your_port"
DB_NAME="your_database"
DB_USER="your_user"
DB_PASSWORD="your_password"

# Table to be exported
TABLE_NAME="your_table_name"
SCHEMA_NAME="your_schema_name"

# Set PGPASSWORD environment variable
export PGPASSWORD=$DB_PASSWORD

# Output SQL file
OUTPUT_FILE="insert_queries.sql"

# Batch size for each query
BATCH_SIZE=100000

# Counter for naming split files
counter=1

# Using pg_dump to generate insert statements with LIMIT and OFFSET
OFFSET=0
while true; do
    pg_dump -h $DB_HOST -p $DB_PORT -U $DB_USER -d $DB_NAME --table=$SCHEMA_NAME.$TABLE_NAME --data-only --column-inserts --no-password --inserts --file=- | psql -h $DB_HOST -p $DB_PORT -U $DB_USER -d $DB_NAME --quiet --no-password --set ON_ERROR_STOP=on --set AUTOCOMMIT=off --single-transaction -c "COPY (SELECT * FROM $SCHEMA_NAME.$TABLE_NAME ORDER BY your_order_column OFFSET $OFFSET LIMIT $BATCH_SIZE) TO STDOUT" >> $OUTPUT_FILE

    # Break the loop if the last query returned fewer rows than the batch size
    [ "$(psql -h $DB_HOST -p $DB_PORT -U $DB_USER -d $DB_NAME -tAc "SELECT COUNT(*) FROM $SCHEMA_NAME.$TABLE_NAME WHERE your_condition")" -lt $BATCH_SIZE ] && break
    OFFSET=$((OFFSET + BATCH_SIZE))

    # Split the output file into smaller files with 100,000 lines each
    split -l 100000 $OUTPUT_FILE "file_$counter"
    
    # Move the split files to have a .sql extension
    for file in file_$counter*; do
        mv "$file" "$file.sql"
    done

    # Increment the counter
    counter=$((counter + 1))
done

#!/bin/bash

# URL to check
URL="https://getdetails.com"

# Check if the URL is up
HTTP_STATUS=$(curl --write-out "%{http_code}" --silent --output /dev/null "$URL")

# If the status code is 200, proceed with the main curl command
if [ "$HTTP_STATUS" -eq 200 ]; then
  echo "URL is up, proceeding with the main request."

  curl --location 'https://getdetails.com/update' \
  --header 'Content-Type: application/json' \
  --data '{ "database" : "ABC", "date" : "2024-06-25"}'
else
  echo "URL is not reachable. Status code: $HTTP_STATUS"
fi



="insert into demo_table (name, id, error_typ, cat_typ, sys_date_Time, wi_number) select name, id, error_typ, cat_typ, current_date, '" & C2 & "' from demo_table where wi_number=" & A2 & ";"

=MID(A1, 8, 2) & "/" & TEXT(MATCH(MID(A1, 10, 3), {"JAN","FEB","MAR","APR","MAY","JUN","JUL","AUG","SEP","OCT","NOV","DEC"},0), "00") & "/20" & MID(A1, 13, 2)
