const jsonString = '{"Scenarios": {"ONG": [{"S_no":1,"name": "John", "age": 30, "city": "New York"},{"S_no":2,"name": "Jane", "age": 25, "city": "Los Angeles"},{"S_no":3,"name": "Bob", "age": 40, "city": "Chicago"}]}}';

try {
  const jsonObject = JSON.parse(jsonString);
  const ongArray = jsonObject.Scenarios.ONG;

  console.log(ongArray); // This will print the entire array of objects
} catch (error) {
  console.error("Error parsing JSON:", error);
}


import { Component, OnInit } from '@angular/core';

@Component({
  selector: 'app-json-display',
  template: `
    <div *ngFor="let keyValue of keyValues">
      <strong>{{ keyValue.key }}:</strong> {{ keyValue.value }}
    </div>
  `,
})
export class JsonDisplayComponent implements OnInit {
  keyValues: { key: string; value: any }[] = [];

  ngOnInit() {
    const jsonString = `
      {
        "results": {
          "premiums": {
            "premium_type": ["REG"],
            "monthly_premium": ["(coverage/1000)*0.0996"]
          }
        }
      }
    `;

    const jsonObject = JSON.parse(jsonString).results;

    for (const key in jsonObject) {
      const value = jsonObject[key];
      this.keyValues.push({ key: key, value: value });
    }
  }
}



========================

const express = require('express');
const router = express.Router();

// Define your route handler function
router.get('/yourRoute', async (req, res) => {
  try {
    // Step 1: Get error records
    const errorRecords = await getErrorRecords();
    
    if (errorRecords.length === 0) {
      console.log("No Error records found");
      return res.status(200).send("No Error records found");
    }
    
    // Step 2: Log in and get JWT
    const token_jwt = await loginJwt();

    // Step 3: Get beta_id details
    const responseData = await getBetaIdDetails(errorRecords);

    // Step 4: Call API for each record
    for (const record of responseData) {
      await callApi(record);
    }

    // Respond with success message
    res.status(200).send("Success");
  } catch (error) {
    console.error("Error:", error);
    res.status(500).send("Internal Server Error");
  }
});

// Define your database queries, API calls, and other functions here
async function getErrorRecords() {
  // Implement your database query logic here
  // Return the error records as an array
  return [];
}

async function loginJwt() {
  // Implement your API call to get JWT here
  // Return the JWT as a string
  return "your_jwt_here";
}

async function getBetaIdDetails(errorRecords) {
  // Implement your database query logic here to fetch beta_id details
  // Map the data to the required format and return an array of responseData
  return [];
}

async function callApi(record) {
  // Implement your API call logic here for each record
  // Check the response and handle errors accordingly
  // You can use libraries like Axios for API calls
  // Example:
  // const response = await axios.post(apiUrl, record);
  // if (response.status === 201 || response.status === 200) {
  //   console.log("Success");
  // } else {
  //   console.error("Error:", response.statusText);
  // }
}

module.exports = router;

========================================

async function getBetaIdDetails(errorRecords) {
  const responseData = [];

  for (const errorRecord of errorRecords) {
    const { client_id, client_type, org_id, status } = errorRecord;

    // Implement your database query logic to fetch beta_id details
    // For example, assuming you have a function fetchBetaIdDetailsByOrgId
    const betaIdDetails = await fetchBetaIdDetailsByOrgId(org_id);

    if (betaIdDetails) {
      const { beta_id, first_name, last_name } = betaIdDetails;

      // Create a payload object for this error record
      const payload = {
        beta_id: beta_id,
        first_name: first_name,
        last_name: last_name,
        client_id: client_id,
        org_id: org_id,
        client_type: client_type,
        status: status,
      };

      // Push the payload object into the responseData array
      responseData.push(payload);
    }
  }

  return responseData;
}



WITH dynamic_query AS (
  SELECT DISTINCT proj_nm, team_nm
  FROM your_dynamic_query_table_or_source
)

SELECT t.*
FROM t_main t
LEFT JOIN dynamic_query dq ON t.proj_nm = dq.proj_nm AND t.team_nm = dq.team_nm
WHERE dq.proj_nm IS NULL AND dq.team_nm IS NULL;


<div class="accordion" id="projectAccordion">
  <div *ngFor="let project of responseData.data.data" class="accordion-item">
    <h2 class="accordion-header" id="heading{{ project.proj_nm }}">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse{{ project.proj_nm }}" aria-expanded="true" aria-controls="collapse{{ project.proj_nm }}">
        {{ project.proj_nm }}
      </button>
    </h2>
    <div id="collapse{{ project.proj_nm }}" class="accordion-collapse collapse" aria-labelledby="heading{{ project.proj_nm }}" data-bs-parent="#projectAccordion">
      <div class="accordion-body">
        <ul>
          <li *ngFor="let category of project.categories">
            {{ category.cat_nm }}: {{ category.flg }}
          </li>
        </ul>
      </div>
    </div>
  </div>
</div>



<div class="accordion accordion-flush" id="projectAccordion">
  <div *ngFor="let project of responseData.data" class="accordion-item">
    <h2 class="accordion-header" id="heading{{ project.proj_nm }}">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse{{ project.proj_nm }}" aria-expanded="true" aria-controls="collapse{{ project.proj_nm }}">
        {{ project.proj_nm }}
      </button>
    </h2>
    <div id="collapse{{ project.proj_nm }}" class="accordion-collapse collapse" aria-labelledby="heading{{ project.proj_nm }}" data-bs-parent="#projectAccordion">
      <div class="accordion-body">
        <ul>
          <li *ngFor="let category of project.categories">
            {{ category.cat_nm }}: {{ category.flg }}
          </li>
        </ul>
      </div>
    </div>
  </div>
</div>


#!/bin/bash

# Define PostgreSQL connection parameters for MirrorDB
MIRROR_DB_HOST="mirror_db_host"
MIRROR_DB_PORT="mirror_db_port"
MIRROR_DB_NAME="MirrorDB"
MIRROR_DB_USER="mirror_db_user"
MIRROR_DB_PASSWORD="mirror_db_password"

# Define PostgreSQL connection parameters for GoldDB
GOLD_DB_HOST="gold_db_host"
GOLD_DB_PORT="gold_db_port"
GOLD_DB_NAME="GoldDB"
GOLD_DB_USER="gold_db_user"
GOLD_DB_PASSWORD="gold_db_password"

# Table to copy
TABLE_NAME="t_demo"

# Dump data from the MirrorDB (without table creation)
PGPASSWORD="$MIRROR_DB_PASSWORD" pg_dump -h $MIRROR_DB_HOST -p $MIRROR_DB_PORT -U $MIRROR_DB_USER -W -a -v -f $TABLE_NAME.dump $MIRROR_DB_NAME --table=$TABLE_NAME

# Truncate the destination table in GoldDB
psql -h $GOLD_DB_HOST -p $GOLD_DB_PORT -U $GOLD_DB_USER -W -d $GOLD_DB_NAME -c "TRUNCATE $TABLE_NAME RESTART IDENTITY;"

# Restore data to the GoldDB (without table creation)
PGPASSWORD="$GOLD_DB_PASSWORD" pg_restore -h $GOLD_DB_HOST -p $GOLD_DB_PORT -U $GOLD_DB_USER --data-only -v $TABLE_NAME.dump -d $GOLD_DB_NAME

# Remove the dump file
rm $TABLE_NAME.dump

echo "Data copy from MirrorDB to GoldDB for table $TABLE_NAME completed."


#!/bin/bash
set -e

# Define PostgreSQL connection parameters for MirrorDB
MIRROR_DB_HOST="mirror_db_host"
MIRROR_DB_PORT="mirror_db_port"
MIRROR_DB_NAME="MirrorDB"
MIRROR_DB_USER="mirror_db_user"
MIRROR_DB_PASSWORD="mirror_db_password"

# Define PostgreSQL connection parameters for GoldDB
GOLD_DB_HOST="gold_db_host"
GOLD_DB_PORT="gold_db_port"
GOLD_DB_NAME="GoldDB"
GOLD_DB_USER="gold_db_user"
GOLD_DB_PASSWORD="gold_db_password"

# Specify the source schema
SOURCE_SCHEMA="public"

# Get a list of tables in the source schema for MirrorDB
MIRROR_TABLES=($(PGPASSWORD="$MIRROR_DB_PASSWORD" psql -h $MIRROR_DB_HOST -p $MIRROR_DB_PORT -U $MIRROR_DB_USER -d $MIRROR_DB_NAME -t -c "SELECT table_name FROM information_schema.tables WHERE table_schema = '$SOURCE_SCHEMA';"))

# Get a list of tables in the source schema for GoldDB
GOLD_TABLES=($(PGPASSWORD="$GOLD_DB_PASSWORD" psql -h $GOLD_DB_HOST -p $GOLD_DB_PORT -U $GOLD_DB_USER -d $GOLD_DB_NAME -t -c "SELECT table_name FROM information_schema.tables WHERE table_schema = '$SOURCE_SCHEMA';"))

# Check if the counts are different and abort if they are
if [ "${#MIRROR_TABLES[@]}" -ne "${#GOLD_TABLES[@]}" ]; then
  echo "Error: The counts of tables in MirrorDB and GoldDB are different. Aborting script."
  exit 1
fi

# Identify tables that are in MirrorDB but not in GoldDB
DIFF_TABLES=()
for TABLE in "${MIRROR_TABLES[@]}"; do
  if [[ ! " ${GOLD_TABLES[@]} " =~ " $TABLE " ]]; then
    DIFF_TABLES+=("$TABLE")
  fi
done

# Iterate over each different table and perform dump and restore
for TABLE_NAME in "${DIFF_TABLES[@]}"; do
  # Dump data from the MirrorDB (custom format)
  PGPASSWORD="$MIRROR_DB_PASSWORD" pg_dump -h $MIRROR_DB_HOST -p $MIRROR_DB_PORT -U $MIRROR_DB_USER -F c -v -f $TABLE_NAME.dump $MIRROR_DB_NAME --table=$SOURCE_SCHEMA.$TABLE_NAME

  # Truncate the destination table in GoldDB
  PGPASSWORD="$GOLD_DB_PASSWORD" psql -h $GOLD_DB_HOST -p $GOLD_DB_PORT -U $GOLD_DB_USER -d $GOLD_DB_NAME -c "TRUNCATE $SOURCE_SCHEMA.$TABLE_NAME CASCADE;"

  # Restore data to the GoldDB (custom format)
  PGPASSWORD="$GOLD_DB_PASSWORD" pg_restore -h $GOLD_DB_HOST -p $GOLD_DB_PORT -U $GOLD_DB_USER -v $TABLE_NAME.dump -d $GOLD_DB_NAME

  # Remove the dump file
  rm $TABLE_NAME.dump

  echo "Data copy from MirrorDB to GoldDB for table $SOURCE_SCHEMA.$TABLE_NAME completed."
done

echo "Data copy from MirrorDB to GoldDB for all different tables completed."

      PGPASSWORD="$GOLD_DB_PASSWORD" psql -h $GOLD_DB_HOST -p $GOLD_DB_PORT -U $GOLD_DB_USER -d $GOLD_DB_NAME -c "CREATE TABLE $SOURCE_SCHEMA.$TABLE AS SELECT * FROM $MIRROR_DB_NAME.$SOURCE_SCHEMA.$TABLE WHERE 1=0;"


#!/bin/bash
set -e

LOG_FOLDER="/path/to/your/log/folder"
LOG_FILE="$LOG_FOLDER/script_log_$(date '+%Y%m%d_%H%M%S').txt"

# Check if the log folder exists, and create it if not
if [ ! -d "$LOG_FOLDER" ]; then
  mkdir -p "$LOG_FOLDER"
fi

# ... (rest of your script)

# Get a list of tables in the source schema for MirrorDB
MIRROR_TABLES=($(PGPASSWORD="$MIRROR_DB_PASSWORD" psql -h $MIRROR_DB_HOST -p $MIRROR_DB_PORT -U $MIRROR_DB_USER -d $MIRROR_DB_NAME -t -c "SELECT table_name FROM information_schema.tables WHERE table_schema = '$SOURCE_SCHEMA';"))

# Get a list of tables in the source schema for GoldDB
GOLD_TABLES=($(PGPASSWORD="$GOLD_DB_PASSWORD" psql -h $GOLD_DB_HOST -p $GOLD_DB_PORT -U $GOLD_DB_USER -d $GOLD_DB_NAME -t -c "SELECT table_name FROM information_schema.tables WHERE table_schema = '$SOURCE_SCHEMA';"))

# Check if the counts are different and log the difference if they are
if [ "${#MIRROR_TABLES[@]}" -ne "${#GOLD_TABLES[@]}" ]; then
  echo "Error: The counts of tables in MirrorDB and GoldDB are different. Aborting script." >> "$LOG_FILE"
  echo "Tables present in MirrorDB but not in GoldDB:" >> "$LOG_FILE"

  # Identify tables that are in MirrorDB but not in GoldDB
  for TABLE in "${MIRROR_TABLES[@]}"; do
    if [[ ! " ${GOLD_TABLES[@]} " =~ " $TABLE " ]]; then
      echo "$TABLE" >> "$LOG_FILE"

      # Dump the table structure and data from MirrorDB to a custom-format file
      PGPASSWORD="$MIRROR_DB_PASSWORD" pg_dump -h $MIRROR_DB_HOST -p $MIRROR_DB_PORT -U $MIRROR_DB_USER -F c -t $SOURCE_SCHEMA.$TABLE $MIRROR_DB_NAME -f $TABLE.dump

      # Restore the table structure and data in GoldDB
      PGPASSWORD="$GOLD_DB_PASSWORD" pg_restore -h $GOLD_DB_HOST -p $GOLD_DB_PORT -U $GOLD_DB_USER -d $GOLD_DB_NAME -v $TABLE.dump

      # Remove the dump file
      rm $TABLE.dump
    fi
  done

  exit 1
fi

# ... (rest of your script)

echo "Data copy from MirrorDB to GoldDB for all different tables completed." >> "$LOG_FILE"


----------------------------------------------------------------------------------------------------------

#!/bin/bash
set -e

LOG_FOLDER="/path/to/your/log/folder"
LOG_FILE="$LOG_FOLDER/script_log_$(date '+%Y%m%d_%H%M%S').txt"

# List of schemas to process
SCHEMAS=("schema1" "schema2" "schema3")

# Number of parallel threads to use
NUM_THREADS=4

# Check if the log folder exists, and create it if not
if [ ! -d "$LOG_FOLDER" ]; then
  mkdir -p "$LOG_FOLDER"
fi

# Function to process a schema
process_schema() {
  local SCHEMA="$1"

  # Get a list of tables in the source schema for MirrorDB
  MIRROR_TABLES=($(PGPASSWORD="$MIRROR_DB_PASSWORD" psql -h $MIRROR_DB_HOST -p $MIRROR_DB_PORT -U $MIRROR_DB_USER -d $MIRROR_DB_NAME -t -c "SELECT table_name FROM information_schema.tables WHERE table_schema = '$SCHEMA';"))

  # Get a list of tables in the source schema for GoldDB
  GOLD_TABLES=($(PGPASSWORD="$GOLD_DB_PASSWORD" psql -h $GOLD_DB_HOST -p $GOLD_DB_PORT -U $GOLD_DB_USER -d $GOLD_DB_NAME -t -c "SELECT table_name FROM information_schema.tables WHERE table_schema = '$SCHEMA';"))

  # Check if the counts are different and log the difference if they are
  if [ "${#MIRROR_TABLES[@]}" -ne "${#GOLD_TABLES[@]}" ]; then
    echo "$(date '+%Y-%m-%d %H:%M:%S') - Error: The counts of tables in MirrorDB and GoldDB for schema $SCHEMA are different. Aborting script." >> "$LOG_FILE"
    echo "$(date '+%Y-%m-%d %H:%M:%S') - Tables present in MirrorDB but not in GoldDB for schema $SCHEMA:" >> "$LOG_FILE"

    # Identify tables that are in MirrorDB but not in GoldDB
    for TABLE in "${MIRROR_TABLES[@]}"; do
      if [[ ! " ${GOLD_TABLES[@]} " =~ " $TABLE " ]]; then
        echo "$(date '+%Y-%m-%d %H:%M:%S') - $TABLE" >> "$LOG_FILE"

        # Dump the table structure and data from MirrorDB to a compressed file
        PGPASSWORD="$MIRROR_DB_PASSWORD" pg_dump -h $MIRROR_DB_HOST -p $MIRROR_DB_PORT -U $MIRROR_DB_USER -F c -Z 9 -t $SCHEMA.$TABLE $MIRROR_DB_NAME -f $TABLE.dump.gz

        # Restore the table structure and data in GoldDB
        gunzip -c $TABLE.dump.gz | PGPASSWORD="$GOLD_DB_PASSWORD" pg_restore -h $GOLD_DB_HOST -p $GOLD_DB_PORT -U $GOLD_DB_USER -d $GOLD_DB_NAME -v -

        # Remove the compressed dump file
        rm $TABLE.dump.gz
      fi
    done

    exit 1
  fi
}

# Export the function to make it available to parallel
export -f process_schema

# Run the process for each schema in parallel
echo "$(date '+%Y-%m-%d %H:%M:%S') - Starting data copy from MirrorDB to GoldDB for all different tables and schemas." >> "$LOG_FILE"
parallel -j $NUM_THREADS process_schema ::: "${SCHEMAS[@]}"
echo "$(date '+%Y-%m-%d %H:%M:%S') - Data copy from MirrorDB to GoldDB for all different tables and schemas completed." >> "$LOG_FILE"
---------------------------------------------------------------------------------------------------------------


#!/bin/bash
set -e

LOG_FOLDER="/path/to/your/log/folder"
LOG_FILE="$LOG_FOLDER/script_log_$(date '+%Y%m%d_%H%M%S').txt"

# List of schemas to process
SCHEMAS=("schema1" "schema2" "schema3")

# Number of parallel threads to use
NUM_THREADS=4

# Check if the log folder exists, and create it if not
if [ ! -d "$LOG_FOLDER" ]; then
  mkdir -p "$LOG_FOLDER"
fi

# Function to process a schema
process_schema() {
  local SCHEMA="$1"

  # Get a list of tables in the source schema for MirrorDB
  MIRROR_TABLES=($(PGPASSWORD="$MIRROR_DB_PASSWORD" psql -h $MIRROR_DB_HOST -p $MIRROR_DB_PORT -U $MIRROR_DB_USER -d $MIRROR_DB_NAME -t -c "SELECT table_name FROM information_schema.tables WHERE table_schema = '$SCHEMA';"))

  # Get a list of tables in the source schema for GoldDB
  GOLD_TABLES=($(PGPASSWORD="$GOLD_DB_PASSWORD" psql -h $GOLD_DB_HOST -p $GOLD_DB_PORT -U $GOLD_DB_USER -d $GOLD_DB_NAME -t -c "SELECT table_name FROM information_schema.tables WHERE table_schema = '$SCHEMA';"))

  # Check if the counts are different and log the difference if they are
  if [ "${#MIRROR_TABLES[@]}" -ne "${#GOLD_TABLES[@]}" ]; then
    echo "$(date '+%Y-%m-%d %H:%M:%S') - Error: The counts of tables in MirrorDB and GoldDB for schema $SCHEMA are different. Aborting script." >> "$LOG_FILE"
    echo "$(date '+%Y-%m-%d %H:%M:%S') - Tables present in MirrorDB but not in GoldDB for schema $SCHEMA:" >> "$LOG_FILE"

    # Identify tables that are in MirrorDB but not in GoldDB
    for TABLE in "${MIRROR_TABLES[@]}"; do
      if [[ ! " ${GOLD_TABLES[@]} " =~ " $TABLE " ]]; then
        echo "$(date '+%Y-%m-%d %H:%M:%S') - $TABLE" >> "$LOG_FILE"

        # Dump the table structure and data from MirrorDB to a compressed file
        PGPASSWORD="$MIRROR_DB_PASSWORD" pg_dump -h $MIRROR_DB_HOST -p $MIRROR_DB_PORT -U $MIRROR_DB_USER -F c -Z 9 -t $SCHEMA.$TABLE $MIRROR_DB_NAME -f $TABLE.dump.gz

        # Restore the table structure and data in GoldDB
        gunzip -c $TABLE.dump.gz | PGPASSWORD="$GOLD_DB_PASSWORD" pg_restore -h $GOLD_DB_HOST -p $GOLD_DB_PORT -U $GOLD_DB_USER -d $GOLD_DB_NAME -v -

        # Remove the compressed dump file
        rm $TABLE.dump.gz
      fi
    done

    exit 1
  fi
}

# Check if the log folder exists, and create it if not
if [ ! -d "$LOG_FOLDER" ]; then
  mkdir -p "$LOG_FOLDER"
fi

# Run the process for each schema in parallel using a for loop
echo "$(date '+%Y-%m-%d %H:%M:%S') - Starting data copy from MirrorDB to GoldDB for all different tables and schemas." >> "$LOG_FILE"
for SCHEMA in "${SCHEMAS[@]}"; do
  process_schema "$SCHEMA" &
done
wait
echo "$(date '+%Y-%m-%d %H:%M:%S') - Data copy from MirrorDB to GoldDB for all different tables and schemas completed." >> "$LOG_FILE"


----


If you want to parallelize the process for multiple schemas, you can modify the script to iterate over multiple schemas and then execute the process for each schema in parallel. Here's an example:

```bash
#!/bin/bash
set -e

# Define PostgreSQL connection parameters for MirrorDB
MIRROR_DB_HOST="mirror_db_host"
MIRROR_DB_PORT="mirror_db_port"
MIRROR_DB_NAME="MirrorDB"
MIRROR_DB_USER="mirror_db_user"
MIRROR_DB_PASSWORD="mirror_db_password"

# Define PostgreSQL connection parameters for GoldDB
GOLD_DB_HOST="gold_db_host"
GOLD_DB_PORT="gold_db_port"
GOLD_DB_NAME="GoldDB"
GOLD_DB_USER="gold_db_user"
GOLD_DB_PASSWORD="gold_db_password"

# Specify the list of source schemas
SOURCE_SCHEMAS=("schema1" "schema2" "schema3")

# Function to perform pg_dump and restore for a schema
function process_schema() {
  local SOURCE_SCHEMA=$1
  local TABLES=$(PGPASSWORD="$MIRROR_DB_PASSWORD" psql -h $MIRROR_DB_HOST -p $MIRROR_DB_PORT -U $MIRROR_DB_USER -d $MIRROR_DB_NAME -t -c "SELECT table_name FROM information_schema.tables WHERE table_schema = '$SOURCE_SCHEMA';")

  local EXISTING_TABLES=$(PGPASSWORD="$GOLD_DB_PASSWORD" psql -h $GOLD_DB_HOST -p $GOLD_DB_PORT -U $GOLD_DB_USER -d $GOLD_DB_NAME -t -c "SELECT table_name FROM information_schema.tables WHERE table_schema = '$SOURCE_SCHEMA';")

  local DELTA_TABLES=()
  for TABLE_NAME in $TABLES; do
    if [[ ! " ${EXISTING_TABLES[@]} " =~ " $TABLE_NAME " ]]; then
      DELTA_TABLES+=("$TABLE_NAME")
    fi
  done

  # Iterate over each table in parallel (using GNU Parallel)
  export -f dump_and_restore
  echo "${DELTA_TABLES[@]}" | tr ' ' '\n' | parallel -j10 dump_and_restore $SOURCE_SCHEMA
}

# Function to perform pg_dump and restore for a table
function dump_and_restore() {
  local SOURCE_SCHEMA=$1
  local TABLE_NAME=$2
  echo "Processing table: $SOURCE_SCHEMA.$TABLE_NAME"
  
  # Dump data from the MirrorDB (custom format)
  PGPASSWORD="$MIRROR_DB_PASSWORD" pg_dump -h $MIRROR_DB_HOST -p $MIRROR_DB_PORT -U $MIRROR_DB_USER -F c -v -f $SOURCE_SCHEMA.$TABLE_NAME.dump $MIRROR_DB_NAME --table=$SOURCE_SCHEMA.$TABLE_NAME

  # Truncate the destination table in GoldDB
  PGPASSWORD="$GOLD_DB_PASSWORD" psql -h $GOLD_DB_HOST -p $GOLD_DB_PORT -U $GOLD_DB_USER -d $GOLD_DB_NAME -c "TRUNCATE $SOURCE_SCHEMA.$TABLE_NAME CASCADE;"

  # Restore data to the GoldDB (custom format)
  PGPASSWORD="$GOLD_DB_PASSWORD" pg_restore -h $GOLD_DB_HOST -p $GOLD_DB_PORT -U $GOLD_DB_USER -v $SOURCE_SCHEMA.$TABLE_NAME.dump -d $GOLD_DB_NAME

  # Remove the dump file
  rm $SOURCE_SCHEMA.$TABLE_NAME.dump

  echo "Data copy from MirrorDB to GoldDB for table $SOURCE_SCHEMA.$TABLE_NAME completed."
}

# Iterate over each schema and perform the process in parallel
export -f process_schema
echo "${SOURCE_SCHEMAS[@]}" | tr ' ' '\n' | parallel -j5 process_schema
```






Certainly! To handle multiple schemas, you can modify the script to iterate over an array of schema names. Here's an updated version:

```bash
#!/bin/bash
set -e

# Define PostgreSQL connection parameters for MirrorDB
MIRROR_DB_HOST="mirror_db_host"
MIRROR_DB_PORT="mirror_db_port"
MIRROR_DB_NAME="MirrorDB"
MIRROR_DB_USER="mirror_db_user"
MIRROR_DB_PASSWORD="mirror_db_password"

# Define PostgreSQL connection parameters for GoldDB
GOLD_DB_HOST="gold_db_host"
GOLD_DB_PORT="gold_db_port"
GOLD_DB_NAME="GoldDB"
GOLD_DB_USER="gold_db_user"
GOLD_DB_PASSWORD="gold_db_password"

# Specify the source schemas
SOURCE_SCHEMAS=("schema1" "schema2" "schema3")

# Function to refresh a table from MirrorDB to GoldDB for a specific schema
function refresh_table() {
  local SOURCE_SCHEMA=$1
  local TABLE_NAME=$2
  echo "Refreshing table: $SOURCE_SCHEMA.$TABLE_NAME"

  # Dump data from the MirrorDB (custom format)
  PGPASSWORD="$MIRROR_DB_PASSWORD" pg_dump -h $MIRROR_DB_HOST -p $MIRROR_DB_PORT -U $MIRROR_DB_USER -F c -v -f $SOURCE_SCHEMA.$TABLE_NAME.dump $MIRROR_DB_NAME --table=$SOURCE_SCHEMA.$TABLE_NAME

  # Truncate the destination table in GoldDB
  PGPASSWORD="$GOLD_DB_PASSWORD" psql -h $GOLD_DB_HOST -p $GOLD_DB_PORT -U $GOLD_DB_USER -d $GOLD_DB_NAME -c "TRUNCATE $SOURCE_SCHEMA.$TABLE_NAME CASCADE;"

  # Restore data to the GoldDB (custom format)
  PGPASSWORD="$GOLD_DB_PASSWORD" pg_restore -h $GOLD_DB_HOST -p $GOLD_DB_PORT -U $GOLD_DB_USER -v $SOURCE_SCHEMA.$TABLE_NAME.dump -d $GOLD_DB_NAME

  # Remove the dump file
  rm $SOURCE_SCHEMA.$TABLE_NAME.dump

  echo "Data copy from MirrorDB to GoldDB for table $SOURCE_SCHEMA.$TABLE_NAME completed."
}

# Export the refresh_table function for parallel processing
export -f refresh_table

# Iterate over each schema
for SOURCE_SCHEMA in "${SOURCE_SCHEMAS[@]}"; do
  # Get a list of tables in the source schema for MirrorDB
  TABLES=$(PGPASSWORD="$MIRROR_DB_PASSWORD" psql -h $MIRROR_DB_HOST -p $MIRROR_DB_PORT -U $MIRROR_DB_USER -d $MIRROR_DB_NAME -t -c "SELECT table_name FROM information_schema.tables WHERE table_schema = '$SOURCE_SCHEMA';")

  # Iterate over each table and perform refresh in parallel
  echo "${TABLES[@]}" | parallel -j10 refresh_table $SOURCE_SCHEMA
done
```

This script defines a loop that iterates over each schema in the `SOURCE_SCHEMAS` array. For each schema, it retrieves the list of tables and then performs the refresh operation in parallel using GNU Parallel. Adjust the connection parameters and schema names as needed for your specific environment.

In this example, the script processes multiple source schemas in parallel, and for each schema, it iterates over the tables in parallel. Adjust the number of parallel jobs (`-j` option) as needed for your system's capabilities.

--




#!/bin/bash
set -e

# Define PostgreSQL connection parameters for MirrorDB
MIRROR_DB_HOST="mirror_db_host"
MIRROR_DB_PORT="mirror_db_port"
MIRROR_DB_NAME="MirrorDB"
MIRROR_DB_USER="mirror_db_user"
MIRROR_DB_PASSWORD="mirror_db_password"

# Define PostgreSQL connection parameters for GoldDB
GOLD_DB_HOST="gold_db_host"
GOLD_DB_PORT="gold_db_port"
GOLD_DB_NAME="GoldDB"
GOLD_DB_USER="gold_db_user"
GOLD_DB_PASSWORD="gold_db_password"

# Specify the source schema
SOURCE_SCHEMA="public"

# Execute the SQL query to get a list of tables with foreign key dependencies
TABLES_WITH_DEPENDENCIES=$(PGPASSWORD="$MIRROR_DB_PASSWORD" psql -h $MIRROR_DB_HOST -p $MIRROR_DB_PORT -U $MIRROR_DB_USER -d $MIRROR_DB_NAME -t -c "
WITH RecursiveDependencies AS (
  SELECT
    conname,
    conrelid::regclass AS table_name,
    confrelid::regclass AS referenced_table
  FROM
    pg_constraint
  WHERE
    confrelid IS NOT NULL
  UNION
  SELECT
    dep.conname,
    dep.conrelid::regclass AS table_name,
    dep.confrelid::regclass AS referenced_table
  FROM
    pg_constraint AS dep
    INNER JOIN RecursiveDependencies AS rd ON dep.conrelid = rd.referenced_table
)
SELECT DISTINCT
  t.table_schema,
  t.table_name
FROM
  information_schema.tables AS t
  LEFT JOIN RecursiveDependencies AS rd ON t.table_name::regclass = rd.table_name
ORDER BY
  rd.conname DESC NULLS FIRST,
  t.table_schema,
  t.table_name;
")

# Check if the counts are different and abort if they are
MIRROR_TABLES=($(PGPASSWORD="$MIRROR_DB_PASSWORD" psql -h $MIRROR_DB_HOST -p $MIRROR_DB_PORT -U $MIRROR_DB_USER -d $MIRROR_DB_NAME -t -c "SELECT table_name FROM information_schema.tables WHERE table_schema = '$SOURCE_SCHEMA';"))
GOLD_TABLES=($(PGPASSWORD="$GOLD_DB_PASSWORD" psql -h $GOLD_DB_HOST -p $GOLD_DB_PORT -U $GOLD_DB_USER -d $GOLD_DB_NAME -t -c "SELECT table_name FROM information_schema.tables WHERE table_schema = '$SOURCE_SCHEMA';"))

if [ "${#MIRROR_TABLES[@]}" -ne "${#GOLD_TABLES[@]}" ]; then
  echo "Error: The counts of tables in MirrorDB and GoldDB are different. Aborting script."
  exit 1
fi

# Loop through the tables and perform dump, truncate, and restore
while read -r TABLE; do
  TABLE_SCHEMA=$(echo "$TABLE" | cut -d ' ' -f 1)
  TABLE_NAME=$(echo "$TABLE" | cut -d ' ' -f 2)

  # Check if the table starts with 'v_'
  if [[ $TABLE_NAME == v_* ]]; then
    echo "Skipping view $TABLE_SCHEMA.$TABLE_NAME."
    continue
  fi

  # Dump data from the MirrorDB (custom format)
  PGPASSWORD="$MIRROR_DB_PASSWORD" pg_dump -h $MIRROR_DB_HOST -p $MIRROR_DB_PORT -U $MIRROR_DB_USER -F c -v -f $TABLE_NAME.dump $MIRROR_DB_NAME --table=$TABLE_SCHEMA.$TABLE_NAME

  # Truncate the destination table in GoldDB
  PGPASSWORD="$GOLD_DB_PASSWORD" psql -h $GOLD_DB_HOST -p $GOLD_DB_PORT -U $GOLD_DB_USER -d $GOLD_DB_NAME -c "TRUNCATE $TABLE_SCHEMA.$TABLE_NAME CASCADE;"

  # Restore data to the GoldDB (custom format)
  PGPASSWORD="$GOLD_DB_PASSWORD" pg_restore -h $GOLD_DB_HOST -p $GOLD_DB_PORT -U $GOLD_DB_USER -v $TABLE_NAME.dump -d $GOLD_DB_NAME

  # Remove the dump file
  rm $TABLE_NAME.dump

  echo "Data copy from MirrorDB to GoldDB for table $TABLE_SCHEMA.$TABLE_NAME completed."
done <<< "$TABLES_WITH_DEPENDENCIES"

echo "Data copy from MirrorDB to GoldDB for all tables completed."



----


WITH RecursiveDependencies AS (
  SELECT
    conname,
    conrelid::regclass AS table_name,
    confrelid::regclass AS referenced_table
  FROM
    pg_constraint
  WHERE
    confrelid IS NOT NULL
)
SELECT DISTINCT
  t.table_schema,
  t.table_name
FROM
  information_schema.tables AS t
  LEFT JOIN RecursiveDependencies AS rd ON t.table_name::regclass = rd.table_name
ORDER BY
  rd.conname DESC NULLS FIRST,
  t.table_schema,
  t.table_name;


-----

WITH RecursiveDependencies AS (
  SELECT
    conname,
    conrelid::regclass AS table_name,
    confrelid::regclass AS referenced_table
  FROM
    pg_constraint
  WHERE
    confrelid IS NOT NULL
    AND conrelid::regclass::text LIKE 'your_schema_name%'
)
SELECT DISTINCT
  t.table_schema,
  t.table_name
FROM
  information_schema.tables AS t
  LEFT JOIN RecursiveDependencies AS rd ON t.table_name::regclass = rd.table_name
WHERE
  t.table_schema = 'your_schema_name' -- Filter by schema
  AND t.table_name IN ('table1', 'table2', 'table3') -- Filter by specific tables if needed
ORDER BY
  t.table_schema,
  rd.conname DESC NULLS FIRST,
  t.table_name;

----


SELECT
  table_name,
  COUNT(conname) AS num_referencing_tables
FROM
  information_schema.tables t
  LEFT JOIN pg_constraint c ON t.table_name::regclass = c.confrelid::regclass
WHERE
  table_schema = 'your_schema_name' -- Replace with your actual schema name
GROUP BY
  t.table_name
ORDER BY
  num_referencing_tables DESC,
  t.table_name;
